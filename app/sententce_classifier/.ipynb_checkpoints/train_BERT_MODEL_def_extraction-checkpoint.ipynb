{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append( \"/notebook/nas-trainings/arne/DGFISMA_definition_extraction/\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME=\"/notebook/nas-trainings/arne/DGFISMA_definition_extraction/DATA/definition_extraction/bootstrapped_training_set/iteration_1/train_dgfisma_wiki.csv\"\n",
    "DELIMITER=\"⚫\"\n",
    "dataset=pd.read_csv( FILENAME, delimiter=DELIMITER, quoting=csv.QUOTE_NONE, header=None, engine='python', names=['text', 'label_tag','label' ])\n",
    "with open(  \"/notebook/nas-trainings/arne/DGFISMA_definition_extraction/DATA/definition_extraction/bootstrapped_training_set/iteration_1/train_sentences.txt\", \"w\" ) as f:\n",
    "    for sentence in dataset.text.tolist():\n",
    "        #f.write( f\"{sentence[1:-1]}\\n\"  )\n",
    "        \n",
    "with open(  \"/notebook/nas-trainings/arne/DGFISMA_definition_extraction/DATA/definition_extraction/bootstrapped_training_set/iteration_1/train_labels.txt\", \"w\" ) as f:\n",
    "    for label in dataset.label.tolist():\n",
    "        #f.write( f\"{label}\\n\"  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/miniconda/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.metrics.classification module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.metrics. Anything that cannot be imported from sklearn.metrics is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertSequenceClassifier: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertSequenceClassifier from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertSequenceClassifier from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertSequenceClassifier were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "seed = 20200922\n",
      "tokenizing...\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/miniconda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "Epoch:   0%|          | 0/10 [00:00<?, ?it/s]epoch: 1\n",
      "Train loss: 0.19384632612019778\n",
      "Validation Accuracy: 0.9553571428571429\n",
      "Validation f1: 0.9557313109455995\n",
      "\n",
      "\n",
      "Epoch:  10%|█         | 1/10 [00:35<05:23, 35.95s/it]epoch: 2\n",
      "Train loss: 0.08563844581693411\n",
      "Validation Accuracy: 0.9680059523809523\n",
      "Validation f1: 0.9679837972922724\n",
      "\n",
      "\n",
      "Epoch:  20%|██        | 2/10 [01:12<04:48, 36.03s/it]epoch: 3\n",
      "Train loss: 0.0704537921771407\n",
      "Validation Accuracy: 0.9702380952380952\n",
      "Validation f1: 0.9702373713732173\n",
      "\n",
      "\n",
      "Epoch:  30%|███       | 3/10 [01:48<04:13, 36.24s/it]epoch: 4\n",
      "Train loss: 0.05959531739912927\n",
      "Validation Accuracy: 0.9709821428571429\n",
      "Validation f1: 0.9708301864457953\n",
      "\n",
      "\n",
      "Epoch:  40%|████      | 4/10 [02:26<03:39, 36.55s/it]epoch: 5\n",
      "Train loss: 0.045674725929275156\n",
      "Validation Accuracy: 0.9717261904761905\n",
      "Validation f1: 0.9717739322330533\n",
      "\n",
      "\n",
      "Epoch:  50%|█████     | 5/10 [03:03<03:04, 36.85s/it]epoch: 6\n",
      "Train loss: 0.037863358459435406\n",
      "Validation Accuracy: 0.9717261904761905\n",
      "Validation f1: 0.9717590181623124\n",
      "\n",
      "\n",
      "Epoch:  60%|██████    | 6/10 [03:41<02:28, 37.12s/it]epoch: 7\n",
      "Train loss: 0.03231934733968228\n",
      "Validation Accuracy: 0.9747023809523809\n",
      "Validation f1: 0.9746667496002901\n",
      "\n",
      "\n",
      "Epoch:  70%|███████   | 7/10 [04:19<01:51, 37.32s/it]epoch: 8\n",
      "Train loss: 0.02659783768467605\n",
      "Validation Accuracy: 0.9702380952380952\n",
      "Validation f1: 0.9703541356968328\n",
      "\n",
      "\n",
      "Epoch:  80%|████████  | 8/10 [04:57<01:14, 37.47s/it]epoch: 9\n",
      "Train loss: 0.02001669339556247\n",
      "Validation Accuracy: 0.9657738095238095\n",
      "Validation f1: 0.9660053715318154\n",
      "\n",
      "\n",
      "Epoch:  90%|█████████ | 9/10 [05:34<00:37, 37.61s/it]epoch: 10\n",
      "Train loss: 0.018705617165891454\n",
      "Validation Accuracy: 0.9724702380952381\n",
      "Validation f1: 0.9725637653455481\n",
      "\n",
      "\n",
      "Epoch: 100%|██████████| 10/10 [06:12<00:00, 37.29s/it]\n",
      "saving model in /notebook/nas-trainings/arne/DGFISMA_definition_extraction/bert_classifier/models_dgfisma_def_extraction/run_2020_12_02_12_51_29_f2f839b04ca9/distilbert-base-uncased_model_10.pth.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/notebook/nas-trainings/arne/DGFISMA_definition_extraction/bert_classifier/models_dgfisma_def_extraction/run_2020_12_02_12_51_29_f2f839b04ca9')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from user_scripts.classifier_train import main\n",
    "\n",
    "train_sentences = \"/notebook/nas-trainings/arne/DGFISMA_definition_extraction/DATA/definition_extraction/bootstrapped_training_set/iteration_1/train_sentences.txt\"\n",
    "train_labels = \"/notebook/nas-trainings/arne/DGFISMA_definition_extraction/DATA/definition_extraction/bootstrapped_training_set/iteration_1/train_labels.txt\"\n",
    "model_storage_directory = \"/notebook/nas-trainings/arne/DGFISMA_definition_extraction/bert_classifier/models_dgfisma_def_extraction\"\n",
    "\n",
    "main(  train_sentences, train_labels, model_storage_directory )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "seed = 20200922\n",
      "tokenizing...\n",
      "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/miniconda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:1944: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "start inference...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " array([[1.4189631e-03, 9.9858105e-01],\n",
       "        [2.6041552e-04, 9.9973959e-01],\n",
       "        [4.3933082e-04, 9.9956065e-01],\n",
       "        [1.4581180e-03, 9.9854195e-01],\n",
       "        [3.7971078e-04, 9.9962032e-01],\n",
       "        [7.1556272e-04, 9.9928445e-01],\n",
       "        [1.6227280e-04, 9.9983776e-01],\n",
       "        [2.1995093e-01, 7.8004909e-01],\n",
       "        [5.5260269e-04, 9.9944741e-01],\n",
       "        [1.6199303e-03, 9.9838006e-01],\n",
       "        [1.6852864e-04, 9.9983144e-01],\n",
       "        [1.7319154e-04, 9.9982685e-01],\n",
       "        [2.5048456e-01, 7.4951541e-01],\n",
       "        [7.1177310e-03, 9.9288225e-01],\n",
       "        [3.1805836e-02, 9.6819413e-01],\n",
       "        [1.8818544e-03, 9.9811816e-01],\n",
       "        [1.9529148e-04, 9.9980468e-01],\n",
       "        [2.8775141e-04, 9.9971217e-01],\n",
       "        [3.6408781e-04, 9.9963593e-01],\n",
       "        [2.7318537e-04, 9.9972683e-01],\n",
       "        [3.3059031e-02, 9.6694094e-01],\n",
       "        [5.3331274e-01, 4.6668726e-01],\n",
       "        [1.2773599e-03, 9.9872261e-01],\n",
       "        [2.0417715e-03, 9.9795818e-01],\n",
       "        [1.0101339e-03, 9.9898988e-01],\n",
       "        [5.3996191e-04, 9.9946004e-01],\n",
       "        [3.9633259e-04, 9.9960369e-01],\n",
       "        [8.9095032e-05, 9.9991095e-01],\n",
       "        [2.5849112e-03, 9.9741507e-01],\n",
       "        [7.8187758e-01, 2.1812241e-01],\n",
       "        [9.9996114e-01, 3.8806818e-05],\n",
       "        [9.9997878e-01, 2.1270378e-05],\n",
       "        [9.9995637e-01, 4.3617249e-05],\n",
       "        [6.0768354e-01, 3.9231649e-01],\n",
       "        [9.9999487e-01, 5.1400339e-06],\n",
       "        [9.9999154e-01, 8.4408030e-06],\n",
       "        [9.9875736e-01, 1.2426920e-03],\n",
       "        [9.9993908e-01, 6.0858292e-05],\n",
       "        [9.9999046e-01, 9.5753248e-06],\n",
       "        [9.9994087e-01, 5.9105307e-05],\n",
       "        [9.9999464e-01, 5.3336976e-06],\n",
       "        [9.9975985e-01, 2.4018281e-04],\n",
       "        [9.9675900e-01, 3.2409795e-03],\n",
       "        [9.9992836e-01, 7.1699069e-05],\n",
       "        [9.9842286e-01, 1.5771466e-03],\n",
       "        [9.9998045e-01, 1.9525176e-05],\n",
       "        [9.9999762e-01, 2.3827379e-06],\n",
       "        [9.9997199e-01, 2.7983175e-05],\n",
       "        [9.9860317e-01, 1.3968879e-03],\n",
       "        [9.9759054e-01, 2.4095159e-03],\n",
       "        [1.8601650e-01, 8.1398350e-01],\n",
       "        [9.8979938e-01, 1.0200678e-02],\n",
       "        [9.9980873e-01, 1.9128587e-04],\n",
       "        [9.9997604e-01, 2.3924558e-05],\n",
       "        [9.9959975e-01, 4.0032252e-04],\n",
       "        [9.9998319e-01, 1.6769442e-05],\n",
       "        [9.9998367e-01, 1.6316189e-05],\n",
       "        [5.6936407e-01, 4.3063590e-01]], dtype=float32))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from user_scripts.classifier_pred import main\n",
    "\n",
    "main( \"/notebook/nas-trainings/arne/DGFISMA_definition_extraction/bert_classifier/models_dgfisma_def_extraction/run_2020_12_02_12_51_29_f2f839b04ca9\",\\\n",
    "    \"/notebook/nas-trainings/arne/DGFISMA_definition_extraction/DATA/definition_extraction/bootstrapped_training_set/iteration_1/test_sentences\", \\\n",
    "    \"/notebook/nas-trainings/arne/DGFISMA_definition_extraction/DATA/definition_extraction/bootstrapped_training_set/iteration_1/test_sentences_pred_test2\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "   y: /notebook/nas-trainings/arne/DGFISMA_definition_extraction/DATA/definition_extraction/bootstrapped_training_set/iteration_1/test_labels\n",
      "pred: /notebook/nas-trainings/arne/DGFISMA_definition_extraction/DATA/definition_extraction/bootstrapped_training_set/iteration_1/test_sentences_pred_test2\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        29\n",
      "           1       0.97      0.97      0.97        29\n",
      "\n",
      "    accuracy                           0.97        58\n",
      "   macro avg       0.97      0.97      0.97        58\n",
      "weighted avg       0.97      0.97      0.97        58\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9655172413793104,\n",
       "  'recall': 0.9655172413793104,\n",
       "  'f1-score': 0.9655172413793104,\n",
       "  'support': 29},\n",
       " '1': {'precision': 0.9655172413793104,\n",
       "  'recall': 0.9655172413793104,\n",
       "  'f1-score': 0.9655172413793104,\n",
       "  'support': 29},\n",
       " 'accuracy': 0.9655172413793104,\n",
       " 'macro avg': {'precision': 0.9655172413793104,\n",
       "  'recall': 0.9655172413793104,\n",
       "  'f1-score': 0.9655172413793104,\n",
       "  'support': 58},\n",
       " 'weighted avg': {'precision': 0.9655172413793104,\n",
       "  'recall': 0.9655172413793104,\n",
       "  'f1-score': 0.9655172413793104,\n",
       "  'support': 58}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from user_scripts.classifier_eval import main\n",
    "\n",
    "path_y = \"/notebook/nas-trainings/arne/DGFISMA_definition_extraction/DATA/definition_extraction/bootstrapped_training_set/iteration_1/test_labels\"\n",
    "path_pred = \"/notebook/nas-trainings/arne/DGFISMA_definition_extraction/DATA/definition_extraction/bootstrapped_training_set/iteration_1/test_sentences_pred_test2\"\n",
    "path_logger = \".\"\n",
    "main(path_y,path_pred,path_logger=path_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
